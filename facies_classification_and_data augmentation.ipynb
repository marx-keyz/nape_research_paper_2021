{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "58768098-04b8-4c36-a941-502f73a504d8",
          "kernelId": ""
        },
        "id": "fLcW_9mrYmSJ"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1FCI32-hs6BlYC_YU7Y7b_PRllSkpIRT-\n",
        "!gdown --id 1fF_zqssPNPsge8TmW1-zoCi1g39EQlda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "9d550420-0253-4021-bc94-47e8e5ca3b5c",
          "kernelId": ""
        },
        "id": "hCDQNhg8Xj-d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install plotly\n",
        "!pip install pytorch_toolbelt # for easy patch-based prediction\n",
        "!pip install albumentations # for data augmentation\n",
        "!pip install torchsummary # for pytorch model architecture summar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "5fb3168d-23a5-4a23-baeb-89fc6b99d096",
          "kernelId": ""
        },
        "id": "umkuUSTOZkOn"
      },
      "outputs": [],
      "source": [
        "# deep learining modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#image preparation module\n",
        "import cv2\n",
        "\n",
        "#data visualization modules\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "411bd51a-0111-4ff3-8f18-511e575a1c6a",
          "kernelId": ""
        },
        "id": "xgV5eqUFbSsJ"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "b5bd2737-1685-4b15-8130-b0ee03f9a775",
          "kernelId": ""
        },
        "id": "3n730UePbYgh"
      },
      "outputs": [],
      "source": [
        "images = np.fromfile('Image_train_1006_777_590.bin', dtype = '>f4')#.newbyteorder('>')\n",
        "labels = np.fromfile('Labels_train_1006_777_590.bin', dtype = 'float32').newbyteorder('>')\n",
        "\n",
        "# # Ensure that dataset is from 0-5 instead of 1-6\n",
        "labels -= 1\n",
        "\n",
        "# check statistic of dataset\n",
        "print(images.min(), images.max(), labels.min(), labels.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "3606da5d-2de2-4489-9a2c-1fdf7e89f4dd",
          "kernelId": ""
        },
        "id": "cFY1Fy-Tiv0c"
      },
      "outputs": [],
      "source": [
        "# Resize\n",
        "images = images.reshape(590, 777, 1006).transpose(2, 1, 0)\n",
        "labels = labels.reshape(590, 777, 1006).transpose(2, 1, 0)\n",
        "images.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "b7eb053d-d4fc-44c7-88f7-44e0d255e282",
          "kernelId": ""
        },
        "id": "9x_JmExkiwFg"
      },
      "outputs": [],
      "source": [
        "train_images = images[:,:, :472]\n",
        "train_labels = labels[:,:, :472]\n",
        "\n",
        "test_images = images[:,:,472:]\n",
        "test_labels = labels[:,:,472:]\n",
        "\n",
        "train_images.shape, test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "90811077-40b0-41bb-b5bc-044d77bf3f8e",
          "kernelId": ""
        },
        "id": "7_HnpDNCixq5"
      },
      "outputs": [],
      "source": [
        "# Pick diagonal slice\n",
        "def ax(arr):\n",
        "    ax1, ax2 = np.diagonal(arr).T, np.fliplr(arr).diagonal().T\n",
        "    ax3, ax4 = np.diagonal(arr, 0,0,-1).T, np.flipud(arr).diagonal(0,0,-1).T\n",
        "    \n",
        "    arr2 = arr[:,:,[i for i in range(0,arr.shape[2])]] = arr[:,:,[i for i in range(arr.shape[2]-1, -1, -1)]]\n",
        "    \n",
        "    ax5, ax6 = np.diagonal(arr2).T, np.fliplr(arr2).diagonal().T\n",
        "    ax7, ax8 = np.diagonal(arr2, 0,0,-1).T, np.flipud(arr2).diagonal(0,0,-1).T\n",
        "    return ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "38de9c0a-76d7-4600-8ad3-3d2dd2a2861f",
          "kernelId": ""
        },
        "id": "Nkh4WWLLixmM"
      },
      "outputs": [],
      "source": [
        "# Create 3d slices from which diagonal slices will be extracted\n",
        "def test_diag(img,lab, ps, amount):\n",
        "    if amount % 8 == 0:\n",
        "        data_test_p = np.zeros([ps,ps,amount*8],dtype='float32')\n",
        "        label_test_p = np.zeros([ps,ps,amount*8],dtype='float32')\n",
        "        b = np.random.randint(low = [0,0,0], high = (np.array(img.shape)-ps).tolist(), size = (amount,3))\n",
        "        for i,row in enumerate(b):\n",
        "            h,x,y = row[0], row[1], row[2] \n",
        "            a = img[h:h+ps,x:x+ps,y:y+ps]\n",
        "            a_lab = lab[h:h+ps,x:x+ps,y:y+ps]\n",
        "            ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8 = ax(a)\n",
        "            ax1_, ax2_, ax3_, ax4_, ax5_, ax6_, ax7_, ax8_ = ax(a_lab)\n",
        "            data_test_p[:,:, i*8:i*8+8] = np.dstack((ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8))\n",
        "            label_test_p[:,:, i*8:i*8+8] = np.dstack((ax1_, ax2_, ax3_, ax4_, ax5_, ax6_, ax7_, ax8_))\n",
        "        return data_test_p, label_test_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "e743bf4e-d2a8-42bb-8fbb-d155dd97723f",
          "kernelId": ""
        },
        "id": "EX60vQDGixhN"
      },
      "outputs": [],
      "source": [
        "# Slice dataset through Y plane\n",
        "def test_crop_x(test_images,test_labels, ps):\n",
        "    lx = int(np.ceil(test_images.shape[0]/ps)*np.ceil(test_images.shape[2]/ps)*test_images.shape[1]) \n",
        "    data_test_p = np.zeros([ps,ps,lx],dtype='float32')\n",
        "    label_test_p = np.zeros([ps,ps,lx],dtype='float32')\n",
        "\n",
        "    xx = 0\n",
        "    for x in range(test_images.shape[1]):\n",
        "        for i in range(int(np.ceil(test_images.shape[0]/ps))):\n",
        "            for j in range(int(np.ceil(test_images.shape[2]/ps))):\n",
        "                if i==int(np.ceil(test_images.shape[0]/ps))-1 and j==int(np.ceil(test_images.shape[2]/ps))-1:\n",
        "                    data_test_p[:,:,xx] = test_images[test_images.shape[0]-ps:,x, test_images.shape[2]-ps:]\n",
        "                    label_test_p[:,:,xx] = test_labels[test_images.shape[0]-ps:,x, test_images.shape[2]-ps:]      \n",
        "                elif i==int(np.ceil(test_images.shape[0]/ps))-1:\n",
        "                    data_test_p[:,:,xx] = test_images[test_images.shape[0]-ps:,x, j*ps:j*ps+ps]\n",
        "                    label_test_p[:,:,xx] = test_labels[test_images.shape[0]-ps:,x,j*ps:j*ps+ps]\n",
        "                elif j==int(np.ceil(test_images.shape[2]/ps))-1: \n",
        "                    data_test_p[:,:,xx] = test_images[i*ps:i*ps+ps,x,test_images.shape[2]-ps:]\n",
        "                    label_test_p[:,:,xx] = test_labels[i*ps:i*ps+ps,x,test_images.shape[2]-ps:]\n",
        "                else:\n",
        "                    data_test_p[:,:,xx] = test_images[i*ps:i*ps+ps,x,j*ps:j*ps+ps]\n",
        "                    label_test_p[:,:,xx] = test_labels[i*ps:i*ps+ps,x,j*ps:j*ps+ps]\n",
        "                xx=xx+1\n",
        "    return data_test_p, label_test_p\n",
        "\n",
        "# Slice dataset through Z plane\n",
        "def test_crop_y(test_images,test_labels, ps):\n",
        "    lx = int(np.ceil(test_images.shape[0]/ps)*np.ceil(test_images.shape[1]/ps)*test_images.shape[2]) \n",
        "    data_test_p = np.zeros([ps,ps,lx],dtype='float32')\n",
        "    label_test_p = np.zeros([ps,ps,lx],dtype='float32')\n",
        "\n",
        "    xx = 0\n",
        "    for x in range(test_images.shape[2]):\n",
        "        for i in range(int(np.ceil(test_images.shape[0]/ps))):\n",
        "            for j in range(int(np.ceil(test_images.shape[1]/ps))):\n",
        "                if i==int(np.ceil(test_images.shape[0]/ps))-1 and j==int(np.ceil(test_images.shape[1]/ps))-1:\n",
        "                    data_test_p[:,:,xx] = test_images[test_images.shape[0]-ps:,test_images.shape[1]-ps:,x]\n",
        "                    label_test_p[:,:,xx] = test_labels[test_images.shape[0]-ps:,test_images.shape[1]-ps:,x]      \n",
        "                elif i==int(np.ceil(test_images.shape[0]/ps))-1:\n",
        "                    data_test_p[:,:,xx] = test_images[test_images.shape[0]-ps:,j*ps:j*ps+ps,x]\n",
        "                    label_test_p[:,:,xx] = test_labels[test_images.shape[0]-ps:,j*ps:j*ps+ps,x]\n",
        "                elif j==int(np.ceil(test_images.shape[1]/ps))-1: \n",
        "                    data_test_p[:,:,xx] = test_images[i*ps:i*ps+ps,test_images.shape[1]-ps:,x]\n",
        "                    label_test_p[:,:,xx] = test_labels[i*ps:i*ps+ps,test_images.shape[1]-ps:,x]\n",
        "                else:\n",
        "                    data_test_p[:,:,xx] = test_images[i*ps:i*ps+ps,j*ps:j*ps+ps,x]\n",
        "                    label_test_p[:,:,xx] = test_labels[i*ps:i*ps+ps,j*ps:j*ps+ps,x]\n",
        "                xx=xx+1\n",
        "    return data_test_p, label_test_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "f3bce760-cc56-4f4d-92b7-615855740fb3",
          "kernelId": ""
        },
        "id": "84lpYdu1ixcb"
      },
      "outputs": [],
      "source": [
        "# Execute slicing for training dataset\n",
        "train_images1, train_labels1 = test_crop_x(train_images, train_labels, 256)\n",
        "train_images2, train_labels2 = test_crop_y(train_images, train_labels, 256)\n",
        "train_images3, train_labels3 = test_diag(train_images, train_labels, 256,128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "a9abd192-8915-40a4-95a1-01ec9f598fbf",
          "kernelId": ""
        },
        "id": "mYUoj97hixXn"
      },
      "outputs": [],
      "source": [
        "# Combine different 2D slice directions to form one combination of training slices\n",
        "train_images = np.dstack((train_images1, train_images2, train_images3))\n",
        "train_labels = np.dstack((train_labels1, train_labels2, train_labels3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "e9ab31ee-8b9b-47e0-a05b-a17fffb12d45",
          "kernelId": ""
        },
        "id": "DhGGDrwojPYB"
      },
      "outputs": [],
      "source": [
        "train_images.shape, train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "c094e995-274f-4a69-bb50-c2f60d024c3c",
          "kernelId": ""
        },
        "id": "Sv1VXIR1ixSt"
      },
      "outputs": [],
      "source": [
        "class seisdataset_train(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super().__init__()\n",
        "        self.X = X.astype('float32')\n",
        "        self.y = y.astype('float32')\n",
        "        self.aug = A.Compose([             \n",
        "            A.ShiftScaleRotate(p=0.35, shift_limit=0, scale_limit=0.30, rotate_limit=30) ,                 \n",
        "            A.HorizontalFlip(p=0.5),                  \n",
        "#             A.RandomCrop(p=1, height=256, width=256),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[2]\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        data = self.X[:,:,index]\n",
        "        label = self.y[:,:,index]\n",
        "        data_aug = self.aug(image=data, mask=label)\n",
        "        data, label = data_aug['image'], data_aug['mask']          \n",
        "        return data[None,:,:], label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "7c3ec30a-7bd1-4751-88d1-212c9c14ba81",
          "kernelId": ""
        },
        "id": "gf4zSJC8ixNk"
      },
      "outputs": [],
      "source": [
        "# Execute Pytorch Data Reader\n",
        "train_data = seisdataset_train(train_images, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "23f6cf90-459f-4ab5-9a95-68fe151ad689",
          "kernelId": ""
        },
        "id": "UOu7mXaVixIy"
      },
      "outputs": [],
      "source": [
        "no_sample = 50\n",
        "data_get = train_data[no_sample]\n",
        "fig, ax = plt.subplots(1,2, sharey=True, figsize=(8,8))\n",
        "ax[0].imshow(data_get[0][0,:,:], cmap='seismic')\n",
        "ax[1].imshow(data_get[1], cmap='Greys')\n",
        "ax[0].set_title('data')\n",
        "ax[1].set_title('label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "b8320eda-801b-43a6-8f4d-e2d4fd7fab2f",
          "kernelId": ""
        },
        "id": "o-x4aIofixEE"
      },
      "outputs": [],
      "source": [
        "#Slice validation set through Z plane\n",
        "def test_crop_test(test_images,test_labels, ps):\n",
        "    lx = int(np.ceil(test_images.shape[0]/ps)*np.ceil(test_images.shape[1]/ps)*test_images.shape[2]) \n",
        "    data_test_p = np.zeros([ps,ps,lx],dtype='float32')\n",
        "    label_test_p = np.zeros([ps,ps,lx],dtype='float32')\n",
        "\n",
        "    xx = 0\n",
        "    for x in range(test_images.shape[2]):\n",
        "        for i in range(int(np.ceil(test_images.shape[0]/ps))):\n",
        "            for j in range(int(np.ceil(test_images.shape[1]/ps))):\n",
        "                if i==int(np.ceil(test_images.shape[0]/ps))-1 and j==int(np.ceil(test_images.shape[1]/ps))-1:\n",
        "                    data_test_p[:,:,xx] = test_images[test_images.shape[0]-ps:,test_images.shape[1]-ps:,x]\n",
        "                    label_test_p[:,:,xx] = test_labels[test_images.shape[0]-ps:,test_images.shape[1]-ps:,x]      \n",
        "                elif i==int(np.ceil(test_images.shape[0]/ps))-1:\n",
        "                    data_test_p[:,:,xx] = test_images[test_images.shape[0]-ps:,j*ps:j*ps+ps,x]\n",
        "                    label_test_p[:,:,xx] = test_labels[test_images.shape[0]-ps:,j*ps:j*ps+ps,x]\n",
        "                elif j==int(np.ceil(test_images.shape[1]/ps))-1: \n",
        "                    data_test_p[:,:,xx] = test_images[i*ps:i*ps+ps,test_images.shape[1]-ps:,x]\n",
        "                    label_test_p[:,:,xx] = test_labels[i*ps:i*ps+ps,test_images.shape[1]-ps:,x]\n",
        "                else:\n",
        "                    data_test_p[:,:,xx] = test_images[i*ps:i*ps+ps,j*ps:j*ps+ps,x]\n",
        "                    label_test_p[:,:,xx] = test_labels[i*ps:i*ps+ps,j*ps:j*ps+ps,x]\n",
        "                xx=xx+1\n",
        "    return data_test_p, label_test_p\n",
        "test_images, test_labels = test_crop_test(test_images, test_labels, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "6ce56539-2cea-4ea0-a80d-74fccca25262",
          "kernelId": ""
        },
        "id": "DMqa-Q4Diw-4"
      },
      "outputs": [],
      "source": [
        "inline=30\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "plt.imshow(test_images[:,:,inline], cmap='seismic')\n",
        "plt.show()\n",
        "plt.imshow(test_labels[:,:,inline],cmap='Greys')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "8042141b-8faa-445f-9f22-6c768b31eb70",
          "kernelId": ""
        },
        "id": "khRShs90iw5j"
      },
      "outputs": [],
      "source": [
        "test_images.shape, test_labels.shape # shape of validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "d65a6afd-7396-47bc-b490-6747d4a850ce",
          "kernelId": ""
        },
        "id": "Bw0ub7hUiwzb"
      },
      "outputs": [],
      "source": [
        "# Load validation data\n",
        "class seisdataset_test(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super().__init__()\n",
        "        self.X = X.astype('float32')\n",
        "        self.y = y.astype('float32')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[2]\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        data = self.X[:,:,index]\n",
        "        label = self.y[:,:,index]\n",
        "        return data[None,:,:], label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "6d3c7111-9b28-4196-b527-6f29f6e4d7ed",
          "kernelId": ""
        },
        "id": "ZkhtJvXZiwhT"
      },
      "outputs": [],
      "source": [
        "batch_size =  32  #training batch size\n",
        "num_epochs = 20    #number of epochs to train\n",
        "num_classes = 6    #number of facies classes\n",
        "learning_rate = 0.001 #learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "8d47e703-9566-413c-b3fa-44a5e1379005",
          "kernelId": ""
        },
        "id": "o_GIBxTOiwQE"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders(batch_size):\n",
        "    train_data = seisdataset_train(train_images, train_labels)\n",
        "    test_data = seisdataset_test(test_images, test_labels)\n",
        "    \n",
        "    train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True, drop_last=True)\n",
        "    test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle=False, drop_last=True)\n",
        "    return train_loader,test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "be4a0286-c8ed-49fd-a1fa-841db7aa4c10",
          "kernelId": ""
        },
        "id": "7XGEm-Ebjn46"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #CPU/GPU selection\n",
        "train_loader,test_loader=get_data_loaders(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "19c224a6-d899-41a8-86c0-d3225ef42cd8",
          "kernelId": ""
        },
        "id": "nbU1cSeYjoCR"
      },
      "outputs": [],
      "source": [
        "# # weights = [0.288128, 0.422128, 0.088654, 0.010016, 0.041389, 0.149684]\n",
        "# # weights = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "# weights = [1, 1, 6, 4, 10, 2]\n",
        "# weights = torch.tensor(weights, dtype=torch.float32)\n",
        "# weights = weights / weights.sum()\n",
        "# print(weights)\n",
        "# weights = 1.0 / weights\n",
        "# weights = weights / weights.sum()\n",
        "# print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "a0b62d92-16a9-4da1-9fb0-f2c95bc2b47d",
          "kernelId": ""
        },
        "id": "wiTtXjYSjoIi"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "model =  smp.Unet(\n",
        "    encoder_name=\"resnet18\", \n",
        "    in_channels=1,                  \n",
        "    classes=6)  \n",
        "\n",
        "#model.load_state_dict(torch.load('model_6_final.ckpt'), strict=False)\n",
        "# model = nn.DataParallel(model) #multi-gpu training\n",
        "model.to(device)\n",
        "\n",
        "#if True:\n",
        "#    params_to_update = []\n",
        "#    for name,param in model.named_parameters():\n",
        "#        if param.requires_grad == True:\n",
        "#            params_to_update.append(param)\n",
        "\n",
        "# weights = torch.FloatTensor(weights).cuda(2)\n",
        "criterion = nn.CrossEntropyLoss() #weight = weights, reduction = 'sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=65) \n",
        "# scaler = torch.cuda.amp.GradScaler(enabled=use_amp) #Mixed precision training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "4b61b396-3e15-453e-8a66-f643bca58e9a",
          "kernelId": ""
        },
        "id": "gKeua-JnjoOT"
      },
      "outputs": [],
      "source": [
        "# from torchsummary import summary\n",
        "# summary(model, (1, 256, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "e20469b1-43fa-4c87-ac1e-8fec3bf40004",
          "kernelId": ""
        },
        "id": "ZDgvQIsxjoSt"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_F1 = []\n",
        "test_F1 = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "train_jacc = []\n",
        "test_jacc = []\n",
        "train_dice = []\n",
        "test_dice = []\n",
        "F1_old = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "84929350-914e-4372-bdba-51f93ce7dc8e",
          "kernelId": ""
        },
        "id": "duC9-83CjoXS"
      },
      "outputs": [],
      "source": [
        "criterion2 = smp.losses.JaccardLoss(\"multiclass\")\n",
        "criterion3 = smp.losses.DiceLoss(\"multiclass\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "c27cf665-6496-416e-bc26-86e9d3e933e3",
          "kernelId": ""
        },
        "id": "PNDWD90Fjobh"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    F1_train = 0.0\n",
        "    acc_train = 0.0\n",
        "    F1_test = 0.0\n",
        "    acc_test = 0.0    \n",
        "    \n",
        "    model.train()\n",
        "    for data, label in train_loader:\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(data.float())\n",
        "        pred = output.data.max(1)[1].cpu().numpy()[:, :, :]\n",
        "        loss = criterion2(output, label.long())\n",
        "        acc = accuracy_score(label.cpu().numpy().flatten(), pred.flatten())\n",
        "        f1s = f1_score(label.cpu().numpy().flatten(), pred.flatten(),average='weighted',zero_division=0)   \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        F1_train += f1s\n",
        "        acc_train += acc\n",
        "        \n",
        "\n",
        "    model.eval()\n",
        "    for data, label in test_loader:\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(data.float())\n",
        "        pred = output.data.max(1)[1].cpu().numpy()[:, :, :]\n",
        "        loss = criterion2(output, label.long())\n",
        "        acc = accuracy_score(label.cpu().numpy().flatten(), pred.flatten())\n",
        "        f1s = f1_score(label.cpu().numpy().flatten(), pred.flatten(),average='weighted',zero_division=0)     \n",
        "\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "        F1_test += f1s\n",
        "        acc_test += acc\n",
        "        \n",
        "\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    F1_train = F1_train/len(train_loader.sampler)*batch_size\n",
        "    acc_train = acc_train/len(train_loader.sampler)*batch_size    \n",
        "    valid_loss = valid_loss/len(test_loader.sampler)*batch_size\n",
        "    F1_test = F1_test/len(test_loader.sampler)*batch_size\n",
        "    acc_test = acc_test/len(test_loader.sampler)*batch_size\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    train_F1.append(F1_train)\n",
        "    test_F1.append(F1_test)\n",
        "    train_acc.append(acc_train)\n",
        "    test_acc.append(acc_test)      \n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tF1_train: {:.6f} \\tF1_test: {:.6f} \\tacc_train: {:.6f} \\tacc_test: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss, F1_train, F1_test, acc_train, acc_test))\n",
        "        \n",
        "    with open('reg_18_no_aug_jaccard.txt', 'a') as f:\n",
        "        f.write('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tF1_train: {:.6f} \\tF1_test: {:.6f} \\tacc_train: {:.6f} \\tacc_test: {:.6f} \\n'.format(\n",
        "        epoch, train_loss, valid_loss, F1_train, F1_test, acc_train, acc_test))\n",
        "\n",
        "    if F1_old < F1_test:\n",
        "        F1_old = F1_test\n",
        "        print('Saving new best model with F1 -', F1_test, '\\n')\n",
        "        torch.save(model.state_dict(), 'resnet18_no_aug_jaccard.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "39dacd05-063c-4679-97ce-7b39b357651d",
          "kernelId": ""
        },
        "id": "vVT6gmQujof-"
      },
      "outputs": [],
      "source": [
        "fig2, ax = plt.subplots(1,3,  figsize=(15,5))\n",
        "ax[1].plot(train_acc)\n",
        "ax[2].plot(train_F1)\n",
        "ax[1].plot(test_acc,'-r')\n",
        "ax[2].plot(test_F1,'-r')\n",
        "ax[1].set_title('Accuracy')\n",
        "ax[2].set_title('F1')\n",
        "ax[1].legend((\"Train\", \"Test\"))\n",
        "ax[2].legend((\"Train\", \"Test\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "70a77fb8-c1af-466e-aa16-193aaaa80ad0",
          "kernelId": ""
        },
        "id": "k2SgIqomjoj9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "6dd3a5d8-7068-49d1-b45d-447b2277f5ca",
          "kernelId": ""
        },
        "id": "LGrTzSjLjooF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "69a8d78c-1010-430d-9113-1d4346e13ca3",
          "kernelId": ""
        },
        "id": "-keSzDc8josO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "68053e88-7f9c-4913-bd07-d2f41e0260d3",
          "kernelId": ""
        },
        "id": "Yn8L4MrhjowU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "e7a7cc7d-790a-4c8a-b03f-8d6203bdb207",
          "kernelId": ""
        },
        "id": "Nheey6yXjo1A"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "432d6ccf-087b-4725-ab53-9e64ad7fe324",
          "kernelId": ""
        },
        "id": "EYGeO2V1jo4H"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "76884485-aad9-4159-970d-11cd196ebcce",
          "kernelId": ""
        },
        "id": "toZOHSnGjo8D"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "c1962705-36a8-415c-93e3-92ba4497f5aa",
          "kernelId": ""
        },
        "id": "qhHwLSmBjpAB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "d7cb5323-a16b-488c-ba25-54411794a9f6",
          "kernelId": ""
        },
        "id": "PlSJHFFTjpDY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gradient": {
          "id": "7d837cce-7ef0-41dc-a3a7-b170a93c54f5",
          "kernelId": ""
        },
        "id": "S2mUWO8ujpGy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SEG_NVIDIA_GPU_HACKATHON.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}